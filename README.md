Если че, весь код и вообще все сгенерено через LLM. 

---

# Генератор бенчмарков и тестер моделей по МКБ-10

Этот проект предоставляет набор инструментов для автоматического создания тестовых наборов (бенчмарков) на основе справочника **Международной классификации болезней 10-го пересмотра (МКБ-10)** и последующего тестирования на них больших языковых моделей (LLM).

## Ключевые возможности

-   **Генерация бенчмарков**: Автоматическое создание вопросов с вариантами ответов на основе данных из `mkb10.csv`.
-   **Три уровня сложности**: Поддержка генерации тестов для разных уровней иерархии МКБ:
    1.  **`block`**: Распознавание крупных блоков диагнозов (например, `A00-B99`).
    2.  **`level1`**: Распознавание 3-значных кодов диагнозов (например, `A01`).
    3.  **`sublevel`**: Распознавание кодов с точкой (например, `A01.0`).
-   **Стандартный формат**: Бенчмарки сохраняются в формате `.csv`, совместимом с популярными фреймворками для оценки моделей (MMLU-подобный формат).
-   **Гибкое тестирование моделей**: Скрипт `test_model.py` позволяет оценивать любую модель, совместимую с OpenAI API (включая локальные модели через LM Studio, Ollama и др.).
-   **Простая настройка**: Все ключевые параметры (API-ключ, эндпоинт, имя модели) легко настраиваются.
-   **Детальная отчетность**: По итогам тестирования выводится понятный отчет с точностью (accuracy) и количеством ошибок.

## Структура проекта

```
.
├── main.py                 # Скрипт для ГЕНЕРАЦИИ бенчмарков
├── test_model.py           # Скрипт для ТЕСТИРОВАНИЯ моделей
├── mkb10.csv               # ИСХОДНЫЙ ФАЙЛ с данными МКБ-10
├── requirements.txt        # Список зависимостей
└── README.md               # Этот файл
```

## Установка и подготовка

1.  **Клонируйте репозиторий** (или просто скачайте файлы).

2.  **Зависимости**:
    ```bash
    pip install -r requirements.txt
    ```

## Порядок работы

Работа с проектом состоит из двух шагов: сначала вы генерируете бенчмарк, а затем используете его для тестирования модели.

### Шаг 1: Генерация бенчмарков (`main.py`)

Этот скрипт читает `mkb10.csv` и создает на его основе файл с тестовыми вопросами.

**Использование:**

Запустите скрипт из командной строки, указав тип задачи (`--task-type`):

-   **Для генерации теста по блокам кодов (A00-B99):**
    ```bash
    python main.py --task-type block
    ```
    *Результат*: Будет создан файл `mkb10_benchmark_block.csv`.

-   **Для генерации теста по 3-значным кодам (A01):**
    ```bash
    python main.py --task-type level1
    ```
    *Результат*: Будет создан файл `mkb10_benchmark_level1.csv`.

-   **Для генерации теста по кодам с точкой (A01.0):**
    ```bash
    python main.py --task-type sublevel
    ```
    *Результат*: Будет создан файл `mkb10_benchmark_sublevel.csv`.

**Либо можно указать без типа задачи, тогда генерируются все задачи.**


### Шаг 2: Тестирование модели (`test_model.py`)

Этот скрипт использует сгенерированный `.csv` файл для оценки производительности языковой модели.

**Перед запуском — настройте скрипт!**

Откройте файл `test_model.py` и измените значения в блоке `НАСТРОЙКИ API И МОДЕЛИ`:

```python
# Адрес API. Для локальных моделей (LM Studio) укажите, например: "http://localhost:1234/v1"
BASE_URL = "https://api.openai.com/v1" 

# Ваш API-ключ. Для локальных моделей можно оставить "not-needed".
# Рекомендуется хранить ключ в переменных окружения.
API_KEY = "YOUR_API_KEY_HERE" 

# Имя модели для тестирования (например, "gpt-4o", "gpt-3.5-turbo")
MODEL_NAME = "gpt-4o"
```

Температуру рекомендуется оставить на 0. 
Количество одновременных запросов `CONCURRENT_REQUESTS` зависит от провайдера.

**Использование:**

Запустите скрипт, указав, какой бенчмарк использовать (`--benchmark-file`):

```bash
python test_model.py --benchmark-file mkb10_benchmark_level1.csv
```

Вы можете подставить имя любого из сгенерированных ранее файлов.

**Либо также не указывать ничего, для запуска со всеми файлами.**
