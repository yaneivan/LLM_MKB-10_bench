Если че, весь код и вообще все сгенерено через LLM. 

TODO:
* Задачи в другие стороны
* Вращение ответов

---

# Генератор бенчмарков и тестер моделей по МКБ-10

Этот проект предоставляет набор инструментов для автоматического создания тестовых наборов (бенчмарков) на основе справочника **Международной классификации болезней 10-го пересмотра (МКБ-10)** и последующего тестирования на них больших языковых моделей (LLM).

## Ключевые возможности

-   **Генерация бенчмарков**: Автоматическое создание вопросов с вариантами ответов на основе данных из `mkb10.csv`.
-   **Три уровня сложности**: Поддержка генерации тестов для разных уровней иерархии МКБ:
    1.  **`block`**: Распознавание крупных блоков диагнозов (например, `A00-B99`).
    2.  **`level1`**: Распознавание 3-значных кодов диагнозов (например, `A01`).
    3.  **`sublevel`**: Распознавание кодов с точкой (например, `A01.0`).
-   **Стандартный формат**: Бенчмарки сохраняются в формате `.csv`, совместимом с популярными фреймворками для оценки моделей (MMLU-подобный формат).
-   **Гибкое тестирование моделей**: Скрипт `test_model.py` позволяет оценивать любую модель, совместимую с OpenAI API (включая локальные модели через LM Studio, Ollama и др.).
-   **Простая настройка**: Все ключевые параметры (API-ключ, эндпоинт, имя модели) легко настраиваются.
-   **Детальная отчетность**: По итогам тестирования выводится понятный отчет с точностью (accuracy) и количеством ошибок.

## Структура проекта

```
.
├── main.py                 # Скрипт для ГЕНЕРАЦИИ бенчмарков
├── test_model.py           # Скрипт для ТЕСТИРОВАНИЯ моделей
├── mkb10.csv               # ИСХОДНЫЙ ФАЙЛ с данными МКБ-10 (нужно добавить самостоятельно)
├── requirements.txt        # Список зависимостей
└── README.md               # Этот файл
```

## Установка и подготовка

1.  **Клонируйте репозиторий** (или просто скачайте файлы).

2.  **Создайте и поместите данные**: Убедитесь, что ваш файл с данными МКБ-10 называется `mkb10.csv` и находится в корневой папке проекта.

3.  **Установите зависимости**: Рекомендуется использовать виртуальное окружение.
    ```bash
    pip install -r requirements.txt
    ```
    Если у вас нет файла `requirements.txt`, создайте его с таким содержимым:
    ```
    openai
    tqdm
    ```
    И выполните команду `pip install -r requirements.txt`.

## Порядок работы

Работа с проектом состоит из двух шагов: сначала вы генерируете бенчмарк, а затем используете его для тестирования модели.

### Шаг 1: Генерация бенчмарков (`main.py`)

Этот скрипт читает `mkb10.csv` и создает на его основе файл с тестовыми вопросами.

**Использование:**

Запустите скрипт из командной строки, указав тип задачи (`--task-type`):

-   **Для генерации теста по блокам кодов (A00-B99):**
    ```bash
    python main.py --task-type block
    ```
    *Результат*: Будет создан файл `mkb10_benchmark_block.csv`.

-   **Для генерации теста по 3-значным кодам (A01):**
    ```bash
    python main.py --task-type level1
    ```
    *Результат*: Будет создан файл `mkb10_benchmark_level1.csv`.

-   **Для генерации теста по кодам с точкой (A01.0):**
    ```bash
    python main.py --task-type sublevel
    ```
    *Результат*: Будет создан файл `mkb10_benchmark_sublevel.csv`.

### Шаг 2: Тестирование модели (`test_model.py`)

Этот скрипт использует сгенерированный `.csv` файл для оценки производительности языковой модели.

**Перед запуском — настройте скрипт!**

Откройте файл `test_model.py` и измените значения в блоке `НАСТРОЙКИ API И МОДЕЛИ`:

```python
# Адрес API. Для локальных моделей (LM Studio) укажите, например: "http://localhost:1234/v1"
BASE_URL = "https://api.openai.com/v1" 

# Ваш API-ключ. Для локальных моделей можно оставить "not-needed".
# Рекомендуется хранить ключ в переменных окружения.
API_KEY = "YOUR_API_KEY_HERE" 

# Имя модели для тестирования (например, "gpt-4o", "gpt-3.5-turbo")
MODEL_NAME = "gpt-4o"
```

**Использование:**

Запустите скрипт, указав, какой бенчмарк использовать (`--benchmark-file`):

```bash
python test_model.py --benchmark-file mkb10_benchmark_level1.csv
```

Вы можете подставить имя любого из сгенерированных ранее файлов.

**Пример отчета в консоли:**

```
Начинаем тестирование модели 'gpt-4o' на файле 'mkb10_benchmark_level1.csv'...
Тестирование модели: 100%|██████████| 2185/2185 [15:20<00:00, 2.37it/s]

--- Отчет о тестировании ---
Модель: gpt-4o
Бенчмарк: mkb10_benchmark_level1.csv
----------------------------
Всего вопросов: 2185
Правильных ответов: 2150
Точность (accuracy): 98.40% (на основе 2185 успешно обработанных вопросов)
Не удалось распознать ответ модели: 0
Ошибок при обращении к API: 0
----------------------------
```

## Расширение системы

Система спроектирована для легкого добавления новых типов задач. Чтобы добавить новый генератор:

1.  Откройте `main.py`.
2.  В классе `MKB10BenchmarkGenerator` добавьте новый метод `generate_task_...()`, который будет реализовывать вашу логику.
3.  Добавьте новый `choice` в `argparse` в конце файла, чтобы можно было вызывать вашу новую задачу из командной строки.